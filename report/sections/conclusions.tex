During this project we trained an agent for playing Pok√©mon battles starting from the classic Reinforcement Learning setup. However, due to the possibly enormous search space, we restricted the pool of possible next states by using a Genetic Algorithm. In particular, we considered \emph{NSGA-II} to find a set of Pareto-optimal turns, based on the player and the opponent's estimated damage and HP. Results show that ParetoPlayer is able to positively bias the training by providing higher rewards. However, when the search space is small enough and a single win condition is presented, Player outperforms ParetoPlayer. Due to a lack of resource availability and time, we were not able to study too in-depth all the components (i.e. we focused more on evaluating the proposed approach rather than searching for the best hyperparameters or network topology). Finally, at the moment the major problems are related to \emph{NSGA-II} time-consuming operation, due to CPU and not GPU computation, and the inability of the trained agent to properly address forced switch, which should be handled separately from another network.