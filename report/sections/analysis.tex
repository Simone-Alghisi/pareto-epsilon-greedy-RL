% statistical results and their significance
In order to assess our experiments we have employed some data analysis techniques, such as statistical tests and plots, to show whether the employment of \emph{NSGA-II} as a warm-up for RL is beneficial or not.

\subsection{Normality} 
The first test we have conducted is concerning the data normality. These tests are used to determine if a data set is well represented by a normal distribution. Moreover, this information can be useful in order to run a \emph{t-test} since it assumes that the measurements are normally distributed.

To fulfil this request, we have employed graphical methods such as \emph{quantile-quantile plot}, which is useful to compare two probability distributions by plotting their quantiles against each other, and two statistical tests such as the \emph{Shapiro-Wilk test}\footnote{\href{https://en.wikipedia.org/wiki/Shapiro\%E2\%80\%93Wilk_test}{https://en.wikipedia.org/wiki/Shapiro-Wilk\_test}} and a \emph{Kolmogorov Smirnov test}\footnote{\href{https://en.wikipedia.org/wiki/Kolmogorov\%E2\%80\%93Smirnov_test}{https://en.wikipedia.org/wiki/Kolmogorov-Smirnov\_test}}. Both statistical tests work by assuming as null hypothesis that the sample distribution is normal, while as alternative hypothesis that sample distribution is not normal. However, Shapiro test is suitable with few observations whereas Kolmogorov is capable of handling more observations.

\subsection{Statistical significance} 
To observe whether Pareto has brought an improvement with respect to classic RL, we have used \emph{box plots}, the \emph{t-test}, and the \emph{Wilcoxon rank mean test}.
Box plots are graphical method to display the data distribution grouping them through their quartiles, while the t-test and Wilcoxon test are two analytical techniques for determining if a difference between two distributions is statistically significant. The t-test assumes normal data, whereas the non-parametric Wilcoxon test can be used with both normal and non-normal data.

\subsection{Empirical Results}
The data we have considered for the analysis of ParetoPlayer and Player are the \emph{episode reward}, namely the average amount of reward the agent gets in a single game, and the \emph{winrate}, which is the percentage of battles won. To this end, we prepared three different 2 VS 2 battles to test the capability of the proposed method: one where both the network and the opponent keep the same teams across the training; one where the teams are still fixed, but there exists a single win condition; and one where the opponent team is sampled randomly from a pool. In particular, for each method, we first performed $10$ training runs with both Player and ParetoPlayer, and then a row-wise mean operation (for each episode, we take the mean of the $10$ rewards) to have two stabler curves for testing purposes.

Regarding the first set of experiments, we found a significant improvement in terms of episode reward for ParetoPlayer w.r.t. Player. As shown by the Kolmogorov-Smirnov, the reward per episode from ParetoPlayer is stochastically greater than the one from Player, meaning that $F_{pp}(x) \leq F_p(x)$ and hence $\mathbb{E}[X_{pp}] \geq \mathbb{E}[X_p] $ with a $p$-value of $p < 2.2 \cdot 10^{-16}$. This trend has been observed also from the box plots when considering $1000$ random battles performed by the agents with the highest reward. In particular, ParetoPlayer's box plot is basically flat, with the majority of points almost equal to the median, while the outliers are more concentrated either nearby or above the median value. Instead, Player's box plot shows that the distribution of the rewards has higher variance with the median slightly lower than the one of ParetoPlayer, and it accounts for more negative values. However, the Wilcoxon rank-sum test, that given $X$ and $Y$ randomly sampled from two populations measures whether the likelihood of $X$ being bigger than $Y$ is equal to the probability of $Y$ being greater than $X$, shows that in some cases the episode reward distribution is shifted to the right, thus greater than the one obtained from the Player (i.e. $p$-value is $2.886 \cdot 10^{-12}$), and in some other they are almost equivalent (i.e. $p$-value of $0.94$). Regarding the win rate, the test has shown that the data distribution of the ParetoPlayer is shifted to the right than the one obtained from Player (i.e. $p$-value of $3.048 \cdot 10^{-5}$).

As it concerns the second set of battles, results show that ParetoPlayer was not able to reach Player's performance. In our opinion, this happened because the search space is relatively small, but most importantly the win condition deviates too much from the most likely suggestions of non-dominated actions. Thus, restricting the search space incorrectly, even for a small number of episodes, leads to worse results than a classic random search.

Regarding 2 VS 2 battles with variable teams, we performed a Wilcoxon Rank Sum Test for two evaluations of ParetoPlayer against a single evaluation of Player. For both tests, we observed that ParetoPlayers' reward distributions have a significant shift location to the right w.r.t to the Player's distribution ($p < 0.002278$ and $p < 0.01391$). Instead, concerning the win rate we observed that: in the first case, the winning percentage is in favour of ParetoPlayer ($0.716$ against $0.694$); in the second case, it is in favour of Player ($0.673$ against $0.694$). This suggests that the advantage Pareto had in 2 VS 2 battles with fixed team has narrowed with variable teams. However, since we have never had the opportunity of reaching training convergence, we cannot draw further conclusions. Nonetheless, Kolmogorov-Smirnov test still shows that the reward per episode from ParetoPlayer is stochastically greater than the one from Player.